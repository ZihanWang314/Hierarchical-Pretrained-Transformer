{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import Tokenizer\n",
    "inputs = torch.load('../condqa_files/data/train_inputs', map_location='cpu')\n",
    "tokenizer = Tokenizer('../condqa_files/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TxtNode, get_level\n",
    "import pickle\n",
    "from functools import reduce\n",
    "import numpy\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import os\n",
    "\n",
    "\n",
    "def split_tokens(tokens):\n",
    "    tokens = tokens.tolist()\n",
    "    cutoffs = []\n",
    "    current_cutoff = []\n",
    "    for idx, i in enumerate(tokens):\n",
    "        if i not in range(50265, 50272):\n",
    "            current_cutoff.append((i, idx))\n",
    "        else:\n",
    "            cutoffs.append(current_cutoff)\n",
    "            current_cutoff = [(i, idx)]\n",
    "    cutoffs.append(current_cutoff)\n",
    "    return cutoffs\n",
    "\n",
    "def create_node_from_document(document_tokens):\n",
    "    base_node = TxtNode([(0, -1)], tokenizer)\n",
    "    nodes = [base_node]\n",
    "    current_node = base_node\n",
    "    for segment in document_tokens:\n",
    "        while current_node.level >= get_level(segment):\n",
    "            current_node = current_node.parent\n",
    "\n",
    "        child = TxtNode(segment, tokenizer)\n",
    "        nodes.append(child)\n",
    "        current_node.children.append(child)\n",
    "        child.parent = current_node\n",
    "        current_node = child\n",
    "    return base_node\n",
    "\n",
    "def get_grouped_tokens(input_ids):\n",
    "    splited_tokens = split_tokens(input_ids)\n",
    "    start_tokens = splited_tokens[0]\n",
    "    document_tokens = splited_tokens[1:-1]\n",
    "    end_tokens = [i for i in splited_tokens[-1] if i[0] != 0]\n",
    "    l_document_tokens = []\n",
    "    while True:\n",
    "        token = end_tokens.pop(0)\n",
    "        if token[0] in list(range(50272, 50279)):\n",
    "            l_document_tokens.append(token)\n",
    "            break\n",
    "        else:\n",
    "            l_document_tokens.append(token)\n",
    "    document_tokens.append(l_document_tokens)\n",
    "    return start_tokens, document_tokens, end_tokens\n",
    "\n",
    "\n",
    "def repeat_a_node(base_node):\n",
    "    nodes = base_node.get_nodes_list()\n",
    "    nodes = [node for node in nodes if node.parent != None]\n",
    "    node = random.choice(nodes)\n",
    "    node_new = node.copy()\n",
    "    parent = node.parent\n",
    "    node_new.parent = parent\n",
    "    position = random.choice(list(range(len(parent.children) + 1)))\n",
    "    parent.children = parent.children[:position] + [node_new] + parent.children[position:]\n",
    "    return base_node\n",
    "\n",
    "def remove_a_node(base_node):\n",
    "    nodes = base_node.get_nodes_list()\n",
    "    nodes = [node for node in nodes if node.parent != None]\n",
    "    node = random.choice(nodes)\n",
    "    parent = node.parent\n",
    "    parent.children.remove(node)\n",
    "    return base_node\n",
    "\n",
    "def mask_a_node(base_node):\n",
    "    nodes = base_node.get_nodes_list()\n",
    "    node = random.choice(nodes)\n",
    "    node.text = [(50264, j) for i, j in node.text]\n",
    "    return base_node\n",
    "\n",
    "def reorder_a_node(base_node):\n",
    "    nodes = base_node.get_nodes_list()\n",
    "    nodes = [node for node in nodes if len(node.children) > 1]\n",
    "    node = random.choice(nodes)\n",
    "    random.shuffle(node.children)\n",
    "    return base_node\n",
    "\n",
    "\n",
    "\n",
    "def recover_index_from_node(node):\n",
    "    nodes = node.get_nodes_list()\n",
    "    text = [i.text for i in nodes][1:]\n",
    "    origin_HTMLelement_index = [i[0][1] for i in text]\n",
    "    text_lengths = [len(i) for i in text]\n",
    "    generated_HTMLelement_index = [0] + numpy.cumsum(text_lengths)[:-1].tolist()\n",
    "    HTMLelement_index = list(zip(origin_HTMLelement_index, generated_HTMLelement_index))\n",
    "    text = reduce(lambda x, y: x + y, text)\n",
    "    return text, HTMLelement_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def contrastive_sampling(input_ids):\n",
    "    start_tokens, document_tokens, end_tokens = get_grouped_tokens(input_ids)\n",
    "    base_node = create_node_from_document(document_tokens)\n",
    "    for i in range(len(base_node.get_nodes_list()) // 20):\n",
    "        base_node = repeat_a_node(base_node)\n",
    "\n",
    "    for i in range(len(base_node.get_nodes_list()) // 20):\n",
    "        base_node = remove_a_node(base_node)\n",
    "\n",
    "    for i in range(len(base_node.get_nodes_list()) // 5):\n",
    "        base_node = reorder_a_node(base_node)\n",
    "\n",
    "    for i in range(len(base_node.get_nodes_list()) // 10):\n",
    "        base_node = mask_a_node(base_node)\n",
    "\n",
    "    document_tokens, HTMLelement_index = recover_index_from_node(base_node)\n",
    "    HTMLelement_index = torch.tensor(HTMLelement_index)\n",
    "    document_tokens = start_tokens + document_tokens + end_tokens\n",
    "    HTMLelement_index[:, 1] += len(start_tokens)\n",
    "\n",
    "    return document_tokens, HTMLelement_index\n",
    "\n",
    "def generate_contrastive_sample(input):\n",
    "    sample = inputs[0]\n",
    "    input_ids = sample[0]\n",
    "    global_mask = sample[1]\n",
    "    attention_mask = sample[2]\n",
    "    mask_HTMLelements = sample[3]\n",
    "    mask_label_HTMLelements = sample[4]\n",
    "    mask_answer_span = sample[5]\n",
    "    qa_id = sample[6]\n",
    "    mask_label_condition = sample[7]\n",
    "\n",
    "    new_input, contrastive_pairs = contrastive_sampling(input_ids)\n",
    "    new_input_ids = torch.tensor([i[0] for i in new_input])\n",
    "    arrangement_index = torch.tensor([i[1] for i in new_input])\n",
    "\n",
    "    global_mask = global_mask[arrangement_index]\n",
    "    attention_mask = attention_mask[arrangement_index]\n",
    "    mask_HTMLelements = mask_HTMLelements[arrangement_index]\n",
    "    mask_label_HTMLelements = mask_label_HTMLelements[arrangement_index]\n",
    "    mask_answer_span = mask_answer_span[arrangement_index]\n",
    "    mask_label_condition = mask_label_condition[arrangement_index]\n",
    "\n",
    "    text_length = new_input_ids.shape[0]\n",
    "    new_input_ids = torch.concat((new_input_ids, torch.ones(4000 - text_length, dtype = torch.long)))\n",
    "    global_mask = torch.concat((global_mask, torch.zeros(4000 - text_length, dtype = torch.bool)))\n",
    "    attention_mask = torch.concat((attention_mask, torch.zeros(4000 - text_length, dtype = torch.bool)))\n",
    "    mask_HTMLelements = torch.concat((mask_HTMLelements, torch.zeros(4000 - text_length, dtype = torch.bool)))\n",
    "    mask_label_HTMLelements = torch.concat((mask_label_HTMLelements, torch.zeros((4000 - text_length, 3), dtype = torch.long)))\n",
    "    mask_answer_span = torch.concat((mask_answer_span, torch.zeros((4000 - text_length, 2), dtype = torch.long)))\n",
    "    mask_label_condition = torch.concat((mask_label_condition, torch.zeros((4000 - text_length, 5, 2), dtype = torch.bool)))\n",
    "\n",
    "    new_sample = [new_input_ids, global_mask, attention_mask, mask_HTMLelements, mask_label_HTMLelements, \\\n",
    "        mask_answer_span, qa_id, mask_label_condition]\n",
    "    return new_sample, contrastive_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = inputs[0]\n",
    "B, pair = generate_contrastive_sample(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_HTML_num = 3\n",
    "ans_indicator = torch.tensor(\n",
    "    [[[0, 1], [0, 0], [0, 0]]]\n",
    ")\n",
    "\n",
    "cond_indicator = torch.tensor(\n",
    "    [[[1, 0], [1, 1], [0, 0]]]\n",
    ")\n",
    "ans_indicator = ans_indicator.transpose(-2, -1)\n",
    "cond_indicator = cond_indicator.transpose(-2, -1)\n",
    "cond_indicator = cond_indicator.unsqueeze(2).repeat(1, 1, max_HTML_num, 1)# 1, 2, 3, 3\n",
    "label_condition = torch.einsum('abcd,abc->acd',cond_indicator, ans_indicator)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24ed5096eae9d1d51f3453e07736a83aafded0970b93f89b73ef375a69b11d68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
